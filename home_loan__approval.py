# -*- coding: utf-8 -*-
"""home_loan _approval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gNRw-iK5SsDRlnZdc9M0Sxc5wOhTu_LL
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import GridSearchCV


df = pd.read_csv('/content/drive/MyDrive/loan_sanction_train.csv')

# Data Exploration
def explore_data(df):
    print("Dataset Shape:", df.shape)
    print("\nData Types:")
    print(df.dtypes)
    print("\nFirst 5 rows:")
    print(df.head())
    print("\nSummary Statistics:")
    print(df.describe())
    print("\nMissing Values:")
    print(df.isnull().sum())

    # Visualize target variable distribution
    plt.figure(figsize=(8, 6))
    sns.countplot(x='Loan_Status', data=df)
    plt.title('Loan Approval Distribution')
    plt.show()

    # Visualize key categorical features
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    sns.countplot(x='Gender', hue='Loan_Status', data=df, ax=axes[0, 0])
    sns.countplot(x='Married', hue='Loan_Status', data=df, ax=axes[0, 1])
    sns.countplot(x='Education', hue='Loan_Status', data=df, ax=axes[1, 0])
    sns.countplot(x='Self_Employed', hue='Loan_Status', data=df, ax=axes[1, 1])
    plt.tight_layout()
    plt.show()

    # Visualize key numerical features
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    sns.boxplot(x='Loan_Status', y='ApplicantIncome', data=df, ax=axes[0, 0])
    sns.boxplot(x='Loan_Status', y='CoapplicantIncome', data=df, ax=axes[0, 1])
    sns.boxplot(x='Loan_Status', y='LoanAmount', data=df, ax=axes[1, 0])
    sns.boxplot(x='Loan_Status', y='Loan_Amount_Term', data=df, ax=axes[1, 1])
    plt.tight_layout()
    plt.show()

# Data Preprocessing
def preprocess_data(df):
    # Make a copy to avoid modifying the original
    df_processed = df.copy()

    # Convert loan status to binary
    df_processed['Loan_Status'] = df_processed['Loan_Status'].map({'Y': 1, 'N': 0})

    # Remove Loan_ID as it's not useful for prediction
    if 'Loan_ID' in df_processed.columns:
        df_processed.drop('Loan_ID', axis=1, inplace=True)

    # Separate features and target
    X = df_processed.drop('Loan_Status', axis=1)
    y = df_processed['Loan_Status']

    # Identify categorical and numerical columns
    categorical_cols = X.select_dtypes(include=['object']).columns
    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns

    # Split the data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    return X_train, X_test, y_train, y_test, categorical_cols, numerical_cols

# Build the model pipeline
def build_model(categorical_cols, numerical_cols):
    # Preprocessing for numerical data
    numerical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])

    # Preprocessing for categorical data
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])

    # Bundle preprocessing for numerical and categorical data
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numerical_transformer, numerical_cols),
            ('cat', categorical_transformer, categorical_cols)
        ])

    # Create the full pipeline with preprocessing and model
    model = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', RandomForestClassifier(random_state=42))
    ])

    return model

# Model evaluation
def evaluate_model(model, X_test, y_test):
    # Make predictions
    y_pred = model.predict(X_test)

    # Evaluation metrics
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred)
    conf_matrix = confusion_matrix(y_test, y_pred)

    print(f"Accuracy: {accuracy:.4f}")
    print("\nClassification Report:")
    print(report)
    print("\nConfusion Matrix:")
    print(conf_matrix)

    # Visualize confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
                xticklabels=['Not Approved', 'Approved'],
                yticklabels=['Not Approved', 'Approved'])
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title('Confusion Matrix')
    plt.show()

# Hyperparameter tuning
def tune_model(model, X_train, y_train):
    # Define the parameter grid
    param_grid = {
        'classifier__n_estimators': [100, 200],
        'classifier__max_depth': [None, 10, 20],
        'classifier__min_samples_split': [2, 5],
        'classifier__min_samples_leaf': [1, 2]
    }

    # Perform grid search
    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')
    grid_search.fit(X_train, y_train)

    print(f"Best Parameters: {grid_search.best_params_}")
    print(f"Best Cross-Validation Score: {grid_search.best_score_:.4f}")

    return grid_search.best_estimator_

# Feature importance analysis
def analyze_feature_importance(model, categorical_cols, numerical_cols):
    # Get feature names after one-hot encoding
    preprocessor = model.named_steps['preprocessor']
    cat_encoder = preprocessor.named_transformers_['cat'].named_steps['onehot']

    # Get the encoded feature names
    encoded_cats = cat_encoder.get_feature_names_out(categorical_cols)
    feature_names = list(numerical_cols) + list(encoded_cats)

    # Get feature importances
    importances = model.named_steps['classifier'].feature_importances_

    # Create a dataframe for better visualization
    feature_imp = pd.DataFrame({
        'Feature': feature_names,
        'Importance': importances
    }).sort_values(by='Importance', ascending=False)

    # Visualize feature importances
    plt.figure(figsize=(12, 8))
    sns.barplot(x='Importance', y='Feature', data=feature_imp.head(15))
    plt.title('Top 15 Feature Importances')
    plt.tight_layout()
    plt.show()

    return feature_imp

# Make predictions on new data
def predict_loan_approval(model, new_data):
    # Prepare the data in the right format
    if isinstance(new_data, dict):
        new_data = pd.DataFrame([new_data])

    # Make prediction
    prediction = model.predict(new_data)
    probability = model.predict_proba(new_data)[:, 1]

    # Return result
    result = {
        'approved': bool(prediction[0]),
        'approval_probability': probability[0]
    }

    return result

# Main function
def main():
    try:
        # Load the dataset
        df = pd.read_csv('/content/drive/MyDrive/loan_sanction_train.csv')

        # Explore the data
        explore_data(df)

        # Preprocess the data
        X_train, X_test, y_train, y_test, categorical_cols, numerical_cols = preprocess_data(df)

        # Build the model
        model = build_model(categorical_cols, numerical_cols)

        # Train the model
        model.fit(X_train, y_train)

        # Evaluate the base model
        print("\nBase Model Evaluation:")
        evaluate_model(model, X_test, y_test)

        # Tune the model
        print("\nHyperparameter Tuning:")
        best_model = tune_model(model, X_train, y_train)

        # Evaluate the tuned model
        print("\nTuned Model Evaluation:")
        evaluate_model(best_model, X_test, y_test)

        # Analyze feature importance
        feature_imp = analyze_feature_importance(best_model, categorical_cols, numerical_cols)
        print("\nFeature Importance:")
        print(feature_imp.head(10))

        # Example of making a prediction
        print("\nExample Prediction:")
        sample = {
            'Gender': 'Male',
            'Married': 'Yes',
            'Dependents': '2',
            'Education': 'Graduate',
            'Self_Employed': 'No',
            'ApplicantIncome': 5000,
            'CoapplicantIncome': 1500,
            'LoanAmount': 150,
            'Loan_Amount_Term': 360,
            'Credit_History': 1,
            'Property_Area': 'Urban'
        }

        result = predict_loan_approval(best_model, sample)
        approval_status = "Approved" if result['approved'] else "Not Approved"
        print(f"Loan Status: {approval_status}")
        print(f"Approval Probability: {result['approval_probability']:.4f}")

    except Exception as e:
        print(f"An error occurred: {e}")

if __name__ == "__main__":
    main()